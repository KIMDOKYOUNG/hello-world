{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib.request #\n",
    "from selenium.webdriver import Chrome\n",
    "import re     \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "#한글 형태소 분석기인 konlpy사용 \n",
    "#이때 konlpy에는 hannanum, kommoran, kkma, twitter이 있는데 해당 분석에서는 kkma사용\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Twitter\n",
    "#apply lambda사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data = pd.read_csv('text_mining.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이모티콘 제거\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#분석에 어긋나는 불용어구 제외 (특수문자, 의성어)\n",
    "han = re.compile(r'[ㄱ-ㅎㅏ-ㅣ!?~,\".\\n\\r#\\ufeff\\u200d]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = []\n",
    "for i in range(len(comment_data)):\n",
    "    comment_list.append(comment_data['comment'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_result = []\n",
    "\n",
    "for i in comment_list:\n",
    "    tokens = re.sub(emoji_pattern,\"\",i)\n",
    "    tokens = re.sub(han,\"\",tokens)\n",
    "    comment_result.append(tokens)\n",
    "\n",
    "comment_result = pd.DataFrame(comment_result, columns=[\"comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "def get_noun(comment_txt):\n",
    "    twitter = Twitter()\n",
    "    noun = []\n",
    "    \n",
    "    if len(comment_txt)>0:\n",
    "        tw = twitter.pos(comment_txt)\n",
    "        for i,j in tw:\n",
    "            if j == 'Noun':\n",
    "                noun.append(i) #명사인 단어만 추출\n",
    "    return noun\n",
    " \n",
    "comment_result['token'] = comment_result['comment'].apply(lambda x: get_noun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "comment_result['token'] = comment_result['comment'].apply(lambda x: get_noun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>자라 슈즈가 이렇게 예쁜거투성이였다구요</td>\n",
       "      <td>[자라, 슈즈, 투성이]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>안녕하세요 이번 영상 편집자 셩입니다정보 오류가 몇군데 있어 댓글 남깁니다자딕앤볼테...</td>\n",
       "      <td>[이번, 영상, 편집자, 셩, 정보, 오류, 군데, 댓글, 딕앤, 볼테르, 룩, 혼란]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>원했던 컨텐츠였는데이렇게 딱 올려주다니 역시 역시 뭘 좀 아시는 센스 사배님 영상 ...</td>\n",
       "      <td>[컨텐츠, 역시, 역시, 뭘, 좀, 아시, 센스, 사배, 영상, 오]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>omg amazing lookbook all the outfits are just ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>메이크업아티스트 사배200만 유투버 사배🤘아이돌 가수 사배연예인 사배🤹♀️모델 사배...</td>\n",
       "      <td>[메이크업, 아티스트, 사배, 유투, 버, 사배, 아이돌, 가수, 사배, 연예인, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0                              자라 슈즈가 이렇게 예쁜거투성이였다구요   \n",
       "1  안녕하세요 이번 영상 편집자 셩입니다정보 오류가 몇군데 있어 댓글 남깁니다자딕앤볼테...   \n",
       "2  원했던 컨텐츠였는데이렇게 딱 올려주다니 역시 역시 뭘 좀 아시는 센스 사배님 영상 ...   \n",
       "3  omg amazing lookbook all the outfits are just ...   \n",
       "4  메이크업아티스트 사배200만 유투버 사배🤘아이돌 가수 사배연예인 사배🤹♀️모델 사배...   \n",
       "\n",
       "                                               token  \n",
       "0                                      [자라, 슈즈, 투성이]  \n",
       "1   [이번, 영상, 편집자, 셩, 정보, 오류, 군데, 댓글, 딕앤, 볼테르, 룩, 혼란]  \n",
       "2             [컨텐츠, 역시, 역시, 뭘, 좀, 아시, 센스, 사배, 영상, 오]  \n",
       "3                                                 []  \n",
       "4  [메이크업, 아티스트, 사배, 유투, 버, 사배, 아이돌, 가수, 사배, 연예인, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v를 활용한 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v참고자료\n",
    "https://programmers.co.kr/learn/courses/21/lessons/1697\n",
    "https://wikidocs.net/22660\n",
    "https://pythonkim.tistory.com/92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = []\n",
    "for i in range(len(comment_result)):\n",
    "    vec2 = []\n",
    "    tm_ls = comment_result['token'].iloc[i]\n",
    "    if len(tm_ls) == 0: #비어있는 리스트 삭제\n",
    "        pass\n",
    "    else:\n",
    "        for j in range(len(tm_ls)): #비어있지 않은 리스트 중에서 단어가 한 음절인 것은 pass\n",
    "            if len(tm_ls[j]) > 1:\n",
    "                vec2.append(tm_ls[j])\n",
    "            else:\n",
    "                pass\n",
    "    if len(vec2) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        vec.append(vec2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['자라', '슈즈', '투성이'],\n",
       " ['이번', '영상', '편집자', '정보', '오류', '군데', '댓글', '딕앤', '볼테르', '혼란'],\n",
       " ['컨텐츠', '역시', '역시', '아시', '센스', '사배', '영상'],\n",
       " ['메이크업',\n",
       "  '아티스트',\n",
       "  '사배',\n",
       "  '유투',\n",
       "  '사배',\n",
       "  '아이돌',\n",
       "  '가수',\n",
       "  '사배',\n",
       "  '연예인',\n",
       "  '사배',\n",
       "  '모델',\n",
       "  '사배',\n",
       "  '아가씨',\n",
       "  '조련사',\n",
       "  '사배'],\n",
       " ['영상', '자라'],\n",
       " ['메이크업', '스타일', '진짜', '미쳣어', '반짝', '빤짝', '악세사리', '거도', '까마귀', '기여'],\n",
       " ['사배',\n",
       "  '정말',\n",
       "  '사랑',\n",
       "  '옛날',\n",
       "  '아프리카티비',\n",
       "  '컨텐츠',\n",
       "  '정말',\n",
       "  '사랑',\n",
       "  '언니',\n",
       "  '정말',\n",
       "  '언니',\n",
       "  '메이크업',\n",
       "  '아티스트',\n",
       "  '사랑'],\n",
       " ['미모', '몸매', '기럭지', '무슨', '일이', '언니', '결혼', '잉잉'],\n",
       " ['사배', '소화', '완전', '사배', '완전'],\n",
       " ['편집자', '능력', '대박'],\n",
       " ['진짜', '포즈', '취하', '천상', '발끝', '언니'],\n",
       " ['세번', '메이크업', '취향'],\n",
       " ['요새', '영상', '힐링', '사랑', '해욤'],\n",
       " ['사배', '헤어스타일', '방법', '앞머리', '부분', '볼륨', '유지'],\n",
       " ['사배',\n",
       "  '정말',\n",
       "  '신경',\n",
       "  '고민',\n",
       "  '학교',\n",
       "  '체육대회',\n",
       "  '키라',\n",
       "  '키라',\n",
       "  '메이크업',\n",
       "  '화장',\n",
       "  '조금',\n",
       "  '익숙',\n",
       "  '치가',\n",
       "  '여름',\n",
       "  '일리',\n",
       "  '블러셔',\n",
       "  '포인트',\n",
       "  '체육대회',\n",
       "  '메이크업',\n",
       "  '해주시',\n",
       "  '항상',\n",
       "  '영상',\n",
       "  '오늘',\n",
       "  '사랑'],\n",
       " ['마지막', '룩북', '진짜', '찰떡', '청순', '청순', '화이트', '원피스'],\n",
       " ['자라', '신발', '맛집', '몰랐넹'],\n",
       " ['언니', '미모', '실화'],\n",
       " ['진짜', '모델', '사랑'],\n",
       " ['오늘', '쏘골져스', '판타스틱', '우리', '언니'],\n",
       " ['시험', '기간', '사배', '영상', '정말'],\n",
       " ['세번', '아홉', '취향', '저격'],\n",
       " ['언니',\n",
       "  '대박',\n",
       "  '그냥',\n",
       "  '뭔가',\n",
       "  '강의',\n",
       "  '표현',\n",
       "  '댓글',\n",
       "  '그냥',\n",
       "  '따리',\n",
       "  '기만',\n",
       "  '매력',\n",
       "  '그냥',\n",
       "  '피어',\n",
       "  '그냥',\n",
       "  '너비'],\n",
       " ['넘사벽', '연예인', '이심', '스타일리시', '화장', '펄펙', '꽃길', '사배'],\n",
       " ['영상', '이사배', '오늘', '기대'],\n",
       " ['언니', '패션', '요즘', '하울'],\n",
       " ['옌옌이셔몸매', '얼굴', '무엇'],\n",
       " ['영성', '수학여행', '렌즈', '참고', '고갈', '언니', '모델'],\n",
       " ['진짜'],\n",
       " ['선미', '사과', '머리', '정말', '쌍둥이', '와우'],\n",
       " ['언니', '모음', '느낌', '언니', '데이트', '느낌', '이제', '자막', '버전', '주시'],\n",
       " ['영상', '캡쳐', '진짜', '대박', '알록달록', '사배', '매력', '여기'],\n",
       " ['우왕', '신경'],\n",
       " ['대박'],\n",
       " ['배당', '여신', '찰떡', '사배'],\n",
       " ['고급'],\n",
       " ['진짜'],\n",
       " ['여름', '해변', '참고', '의상', '소품', '악세사리'],\n",
       " ['사배'],\n",
       " ['몸매', '일이', '허벅지', '진짜'],\n",
       " ['사배'],\n",
       " ['언니', '진짜'],\n",
       " ['슈스'],\n",
       " ['이름', '사배', '일본'],\n",
       " ['언니', '영상', '매번', '레전드', '감히', '레전드', '오브', '레전드'],\n",
       " ['자막', '나중', '보고', '보물', '사배', '생각', '마상'],\n",
       " ['진짜', '영상', '댓글', '처음', '진짜'],\n",
       " ['사배', '혹시', '기본', '항상', '착용', '피어싱', '정보'],\n",
       " ['싸뢍', '오오오오', '아가씨'],\n",
       " ['즈음', '코코넛', '썬글라스'],\n",
       " ['진짜', '심쿵', '유발', '진짜', '언니'],\n",
       " ['알흠', '다운', '여왕', '여신', '자체', '당신'],\n",
       " ['어멈머', '인스타', '영상', '바로'],\n",
       " ['오예', '오예', '룩북'],\n",
       " ['언닝', '기간', '메이크업', '올려쥬시', '안대', '나용'],\n",
       " ['사배', '위기', '끝장', '모델', '저리', '가라이', '패션', '분위기'],\n",
       " ['자라', '신발', '가요', '몇번', '발아', '파서'],\n",
       " ['유언', '모낭', '때문', '고생'],\n",
       " ['언니', '렌즈', '모델', '해도', '렌즈', '찰떡', '언니', '스타일', '진짜'],\n",
       " ['언니', '진짜', '너비'],\n",
       " ['우왕', '알림'],\n",
       " ['언니', '진짜', '행복', '기분'],\n",
       " ['언니', '진짜', '요정'],\n",
       " ['오늘', '이사배', '이사배', '사배'],\n",
       " ['마지막', '가면'],\n",
       " ['다음', '룩북', '세로', '안구', '정화'],\n",
       " ['언니', '자라', '슈즈', '진짜', '여름'],\n",
       " ['벙얌'],\n",
       " ['한국', '브랜드', '하얀색', '드레스', '완전', '스타일'],\n",
       " ['미모', '사배', '더더'],\n",
       " ['역시', '패션', '완성', '얼굴', '소화', '사배'],\n",
       " ['포드', '진짜', '볼드', '언니', '찰떡'],\n",
       " ['크아', '언니', '포드', '수인', '크아'],\n",
       " ['정말'],\n",
       " ['세번', '메이크업', '언니', '영상'],\n",
       " ['뭔가'],\n",
       " ['블랙', '스팽글', '드레스', '자막', '오류'],\n",
       " ['언닝', '패션', '하울', '진짜'],\n",
       " ['언니', '노래', '제목'],\n",
       " ['와웅', '진짜', '핵패셔너블'],\n",
       " ['두번째', '소개', '화이트', '자켓', '드레스', '블랙', '스팽', '드레스', '잘못'],\n",
       " ['여신', '크으룩북', '대리'],\n",
       " ['자라',\n",
       "  '신발',\n",
       "  '깜놀',\n",
       "  '패션',\n",
       "  '감각',\n",
       "  '패션',\n",
       "  '콘텐츠',\n",
       "  '좀더',\n",
       "  '업로드',\n",
       "  '언니',\n",
       "  '정말',\n",
       "  '노력',\n",
       "  '훨훨',\n",
       "  '모습',\n",
       "  '롤모델',\n",
       "  '응원'],\n",
       " ['언니'],\n",
       " ['포드', '원피스', '가방', '자라', '원피스'],\n",
       " ['사배', '사실', '공주'],\n",
       " ['요런', '룩북', '영상', '가끔', '업로드', '이번', '폭발'],\n",
       " ['사배', '다리', '정말', '너비'],\n",
       " ['진짜', '스타', '일링', '영상', '화장'],\n",
       " ['진짜', '도대체', '언니', '매력', '어디', '도대체', '제발'],\n",
       " ['포드', '메이크업'],\n",
       " ['사배', '약간', '메시', '웨이브', '진짜', '정말', '홀딱', '정말', '엉엉', '언니', '사랑'],\n",
       " ['영상', '미가', '어무'],\n",
       " ['패완얼', '화장'],\n",
       " ['포드', '제품', '몇개', '역시', '사배'],\n",
       " ['언니', '진짜', '미모', '실화', '사배'],\n",
       " ['브금', '제목', '뭔가'],\n",
       " ['사배', '사랑', '당신', '최고'],\n",
       " ['사배'],\n",
       " ['사배', '인스타', '통해', '출장', '영상', '하나', '하나', '신경', '감격', '판타스틱'],\n",
       " ['메이크업', '북도'],\n",
       " ['언니', '자녀'],\n",
       " ['혹시',\n",
       "  '기억',\n",
       "  '어제',\n",
       "  '자라',\n",
       "  '사진',\n",
       "  '달라',\n",
       "  '쇼핑',\n",
       "  '민폐',\n",
       "  '어유',\n",
       "  '인사',\n",
       "  '사랑',\n",
       "  '새내기',\n",
       "  '모든',\n",
       "  '화장',\n",
       "  '언니',\n",
       "  '얼굴',\n",
       "  '진짜',\n",
       "  '언니',\n",
       "  '화면',\n",
       "  '셀럽',\n",
       "  '자체'],\n",
       " ['증말', '채널', '편집자', '센스'],\n",
       " ['오늘', '선미', '자꾸'],\n",
       " ['사배', '몸매', '관리', '법좀', '각선미'],\n",
       " ['진짜', '룩들', '역대'],\n",
       " ['인생'],\n",
       " ['언니'],\n",
       " ['언니', '인스타', '타이밍', '대박', '언니', '사랑'],\n",
       " ['부제', '이사배', '입덕', '영상'],\n",
       " ['화면', '전환'],\n",
       " ['자라', '신발'],\n",
       " ['얼굴', '선도', '찰떡', '혹시', '운동', '식단', '관리', '소개'],\n",
       " ['언니', '자라', '찰떡', '소화', '사배'],\n",
       " ['화보'],\n",
       " ['언니', '오늘', '우상'],\n",
       " ['사배', '은줄'],\n",
       " ['메이크업'],\n",
       " ['사랑', '케이'],\n",
       " ['오늘', '완죤', '이세'],\n",
       " ['사배'],\n",
       " ['사배', '최고', '그냥', '연예인'],\n",
       " ['사배', '메이크업', '튜토리얼'],\n",
       " ['연예인', '완전'],\n",
       " ['편집', '진짜'],\n",
       " ['언니'],\n",
       " ['의상', '악세사리', '하나', '하나', '어울림', '사배', '정말', '젤예', '다음', '영상'],\n",
       " ['장면', '한국', '갈라고']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vec, min_count=3,window=3,iter=20, size=100, sg=1) #skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('매력', 0.27373650670051575),\n",
       " ('진짜', 0.2653716802597046),\n",
       " ('오늘', 0.23340165615081787),\n",
       " ('그냥', 0.1889837384223938),\n",
       " ('혹시', 0.18146313726902008),\n",
       " ('하나', 0.15835776925086975),\n",
       " ('정말', 0.13851365447044373),\n",
       " ('세번', 0.13013607263565063),\n",
       " ('모델', 0.11384862661361694),\n",
       " ('패션', 0.10824351012706757)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('언니')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18601894"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('메이크업', '얼굴')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\tlsal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "model.wv.syn0.shape #w2v size  -> 45개의 단어가 100차원을 가지고 있음\n",
    "\n",
    "#45개의 단어가 100차원으로 벡터화된 수치를 넣어줌\n",
    "word_vectors = model.wv.syn0 # model.wv.vectors와 같은 값을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00221818, -0.00446184, -0.00568183, ..., -0.00201449,\n",
       "        -0.00435981, -0.00475323],\n",
       "       [ 0.00161569, -0.0026832 , -0.00140384, ...,  0.00056937,\n",
       "        -0.00116363, -0.00492794],\n",
       "       [-0.00497487, -0.00099314, -0.00389601, ...,  0.00275114,\n",
       "        -0.00508804, -0.00019692],\n",
       "       ...,\n",
       "       [ 0.00358225, -0.00460652,  0.00293176, ..., -0.00297299,\n",
       "         0.00418551, -0.0006012 ],\n",
       "       [ 0.00437588, -0.00087983, -0.00112393, ..., -0.00034482,\n",
       "         0.0021601 ,  0.00135014],\n",
       "       [ 0.00155235,  0.00013301, -0.0038275 , ..., -0.00057936,\n",
       "         0.00332254,  0.00450196]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 6 #cluster의 갯수 산정\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )#default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = kmeans_clustering.fit_predict( word_vectors ) #각 단어가 어떤 군집에 속하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 0, 5, 2, 4, 1, 4, 3, 5, 0, 0, 2, 5, 2, 5, 4, 5, 2, 1, 5, 5,\n",
       "       5, 2, 3, 2, 1, 1, 1, 5, 2, 4, 3, 1, 1, 2, 2, 3, 1, 1, 3, 1, 4, 5,\n",
       "       2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 어휘 단어를 클러스터 번호에 매핑되게 word/Index 사전을 만든다.\n",
    "idx = list(idx)\n",
    "names = model.wv.index2word #학습된 단어들 unique\n",
    "word_centroid_map = {names[i]: idx[i] for i in range(len(names))}#각 단어 별로 클러스터 번호에 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사배', '언니', '진짜', '영상', '메이크업', '정말', '사랑', '자라', '오늘', '찰떡', '패션', '모델', '대박', '화장', '그냥', '포드', '역시', '연예인', '미모', '완전', '룩북', '신발', '이사배', '얼굴', '드레스', '하나', '편집자', '댓글', '스타일', '악세사리', '몸매', '소화', '세번', '신경', '여름', '원피스', '뭔가', '매력', '너비', '렌즈', '자막', '여신', '레전드', '혹시', '인스타']\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['언니', '진짜', '패션', '모델']\n",
      "\n",
      "Cluster 1\n",
      "['사랑', '완전', '편집자', '댓글', '스타일', '신경', '여름', '너비', '렌즈', '여신']\n",
      "\n",
      "Cluster 2\n",
      "['메이크업', '대박', '그냥', '미모', '얼굴', '하나', '몸매', '원피스', '뭔가', '인스타']\n",
      "\n",
      "Cluster 3\n",
      "['오늘', '드레스', '세번', '매력', '자막']\n",
      "\n",
      "Cluster 4\n",
      "['사배', '정말', '자라', '역시', '소화', '레전드']\n",
      "\n",
      "Cluster 5\n",
      "['영상', '찰떡', '화장', '포드', '연예인', '룩북', '신발', '이사배', '악세사리', '혹시']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,6):\n",
    "    # 클러스터 번호를 출력\n",
    "    print(\"\\nCluster {}\".format(cluster))\n",
    "    \n",
    "    # 클러스터번호와 클러스터에 있는 단어를 찍는다.\n",
    "    words = []\n",
    "    for i in range(0,len(list(word_centroid_map.values()))): #list(dict)키 값만 리스트로 반환\n",
    "        #list(dict.values)는 value값이 리스트로 반환\n",
    "        if( list(word_centroid_map.values())[i] == cluster ):\n",
    "            words.append(list(word_centroid_map.keys())[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
